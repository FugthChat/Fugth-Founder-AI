import os
import json
import logging
import threading
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request
from sse_starlette.sse import EventSourceResponse
from pydantic import BaseModel
from llama_cpp import Llama
from huggingface_hub import hf_hub_download
from fastapi.middleware.cors import CORSMiddleware

# --- Basic Setup ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Model & State Management ---
# This dictionary maps your model names to actual GGUF files.
MODELS = {
    "hanh": {
        "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
        "filename": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "info": "Good for everyday tasks."
    },
    "ndm": {
        "repo_id": "TheBloke/CodeLlama-7B-Instruct-GGUF",
        "filename": "codellama-7b-instruct.Q4_K_M.gguf",
        "info": "Specialized for Python and HTML coding."
    },
    "p8": {
        "repo_id": None, # Placeholder for a real image model
        "filename": None,
        "info": "Image generation model (feature in development)."
    },
    "np": {
        "repo_id": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
        "filename": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "info": "Advanced custom model with RAG capabilities (feature in development)."
    }
}

# Global state variables
LLM = None
CURRENT_MODEL_ID = None
model_lock = threading.Lock()

def load_model(model_id: str):
    """
    Loads a model into memory. If a different model is already loaded,
    it unloads it first. This is crucial for managing memory on the free tier.
    """
    global LLM, CURRENT_MODEL_ID
    
    with model_lock:
        if CURRENT_MODEL_ID == model_id and LLM is not None:
            logger.info(f"Model '{model_id}' is already loaded.")
            return True

        if LLM is not None:
            logger.info(f"Unloading model '{CURRENT_MODEL_ID}' to load '{model_id}'...")
            LLM = None
            CURRENT_MODEL_ID = None
        
        model_config = MODELS.get(model_id)
        if not model_config or not model_config.get("repo_id"):
            logger.warning(f"No valid configuration for model_id: {model_id}")
            return False

        model_path = os.path.join("models", model_config["filename"])
        
        if not os.path.exists(model_path):
            logger.info(f"Downloading model '{model_config['filename']}'...")
            try:
                hf_hub_download(repo_id=model_config["repo_id"], filename=model_config["filename"], local_dir="models")
            except Exception as e:
                logger.error(f"Failed to download model '{model_id}': {e}")
                return False
        
        logger.info(f"Loading model '{model_id}' from path: {model_path}")
        try:
            LLM = Llama(model_path=model_path, n_ctx=2048, n_threads=4, n_gpu_layers=0, verbose=True)
            CURRENT_MODEL_ID = model_id
            logger.info(f"Model '{model_id}' loaded successfully.")
            return True
        except Exception as e:
            logger.error(f"Failed to load model '{model_id}': {e}")
            return False

# --- FastAPI App Setup ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Allows your GitHub Pages site to connect
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- API Request Models ---
class ChatRequest(BaseModel):
    model_id: str
    prompt: str
    line_count: str = "default"

# --- API Endpoints ---
@app.get("/")
def get_status():
    return {"status": "AI server is online", "current_model": CURRENT_MODEL_ID}

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    
    # Handle special placeholder models first
    if request.model_id == "p8":
        async def placeholder_generator_p8():
            yield f"data: {json.dumps({'content': 'ðŸŽ¨ Feature in development: Image generation powered by advanced models is coming soon.'})}\n\n"
        return EventSourceResponse(placeholder_generator_p8())
        
    if request.model_id == "np":
        async def placeholder_generator_np():
            yield f"data: {json.dumps({'content': 'ðŸ“š Feature in development: Advanced RAG capabilities for custom data analysis are coming soon.'})}\n\n"
        return EventSourceResponse(placeholder_generator_np())

    # Load the requested model if it's not already loaded
    if not load_model(request.model_id):
        async def error_generator():
            yield f"data: {json.dumps({'error': 'Failed to load the selected AI model. Please try again or select a different model.'})}\n\n"
        return EventSourceResponse(error_generator())
    
    # Modify the prompt for the coding model
    final_prompt = request.prompt
    if request.model_id == "ndm" and request.line_count != "default":
        line_instruction = f"Please generate approximately {request.line_count} lines of code."
        final_prompt = f"{request.prompt}\n\n[Instruction: {line_instruction}]"
        logger.info(f"Added coding instruction: {line_instruction}")

    async def event_generator():
        try:
            stream = LLM.create_completion(
                final_prompt,
                max_tokens=2048, # Increased token limit for longer code
                temperature=0.7,
                stop=["</s>", "<|user|>", "<|system|>"],
                stream=True
            )
            for chunk in stream:
                content = chunk['choices'][0]['text']
                if content:
                    yield f"data: {json.dumps({'content': content})}\n\n"
        except Exception as e:
            logger.error(f"Error during generation: {e}")
            yield f"data: {json.dumps({'error': 'An error occurred during text generation.'})}\n\n"
    
    return EventSourceResponse(event_generator())
